**Home Credit Default Risk - Prediction**
**2주차 주제: Boosting 기법 (LightGBM, XGBoost, CatBoost)
Dataset:**  [Home Credit Default Risk](https://www.kaggle.com/competitions/home-credit-default-risk)
**커널 필사**:  [Introduction to Manual Feature Engineering](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering)
                  [Stacking(sklearn-xgboost-catboost-lightgbm)](https://www.kaggle.com/code/eliotbarr/stacking-test-sklearn-xgboost-catboost-lightgbm)

**1-1. 이론 공부 1 (배지원) : Boosting 기법**

[Intro to Manual Feature Engineering](https://www.notion.so/Intro-to-Manual-Feature-Engineering-9d515abc11d34f71a324c6010f8b77bb?pvs=21)

**1-2. 이론 공부 2 (정하연) : K-fold cross validation, LightGBM**

[2. Binary Classification - Tabular Data 이론](https://www.notion.so/2-Binary-Classification-Tabular-Data-ab5570e7cdbd4875aca15d7e0c1e09b4?pvs=21)

[2. Binary Classification - Tabular Data 이론](https://www.notion.so/2-Binary-Classification-Tabular-Data-ab5570e7cdbd4875aca15d7e0c1e09b4?pvs=21)

**2-1. 코드 필사 1 (배지원, 이동주) : EDA + Feature Engineering** 

**2-2. 코드 필사 2 (정하연, 강지윤) : Stacking (LightGBM,XGBOOST,CATBOOST,Extra trees classifier, Random Forest →  Logistic regression)**
